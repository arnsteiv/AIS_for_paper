{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arbeidsbok: Nye stats\n",
    "\n",
    "Du arbeider her for å videreføre arbeidet\n",
    "\n",
    "Du har gjort:\n",
    "- Beregnet steady-state course- og speed change\n",
    "- Beregnet forholdet, ikke bare true/false når det kommer til dSOG og dCOG og hvor vidt de overstiger verdier for før-perioden (overshoot)\n",
    "- Du har begrenset før- og etter-periodene til 60 sek før og etter ikke alle observasjoner før og etter\n",
    "- Du har sørget for å bruke circmean istedenfor mean på sirkulære verdier (viktig)\n",
    "- Du beregner avstanden mellom skipene idet de foretar sin manøver\n",
    "- Du gjør utvelgelse ift lengde på skip, ikke med absolutt størrelse\n",
    "- Du henter ut lats og lons for situasjonene, slik at du kan plotte dem\n",
    "\n",
    "Du kan fremdeles:\n",
    "- Ha en høyere begrensnning for hva det vil si å registreres som en speed/course maneuver (prøv deg frem med dette og plott i R)\n",
    "- Leke deg rundt med om mean eller max er best for å bestemme course/speed maneuver\n",
    "- NB: Husk å ta ut no action-ene, eventuelt bruke steady-state og total speed change / coruse change til å teste robustheten til disse\n",
    "- Bruke mean rolling filter for steady course / speed change isteden\n",
    "- Plotte dSOG, SOG osv for noen utvalgte situasjoner - se at ting stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing relevant libraries\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import datetime as dt\n",
    "import pyarrow.parquet as pq\n",
    "import glob, os\n",
    "\n",
    "from NearCollisionStatistics import *\n",
    "\n",
    "# Setting internal options for speedup\n",
    "pd.set_option('compute.use_bottleneck', True)\n",
    "pd.set_option('compute.use_numexpr', True)\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading small dataset from CSV\n",
    "\n",
    "Reading the small one-day sample from CSV file. This does not take increadibly much memory (but still quite some)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Building a databank ###\n",
    "\n",
    "# Re-reading full AIS original file\n",
    "PATH_csv = \"~/code/DATA_DNV/ais_small.csv\"\n",
    "\n",
    "incols = [\"date_time_utc\",\"mmsi\",\"lat\",\"lon\",\"sog\",\"cog\",\"true_heading\",\"length\",\"nav_status\",\"RISK_Norwegian_Main_Vessel_Category_ID\"]\n",
    "\n",
    "AIS_df = pd.io.parsers.read_csv(PATH_csv\n",
    "                        ,engine=\"c\"\n",
    "                        ,sep=\";\"\n",
    "                        ,usecols=incols        \n",
    "                        ,compression=None\n",
    "                        ,dtype={\"mmsi\": np.uint64,             \n",
    "                                \"lat\": np.float32, \n",
    "                                \"lon\": np.float32,\n",
    "                                \"sog\": np.float32,\n",
    "                                \"cog\": np.float32,\n",
    "                                \"true_heading\": np.float32,\n",
    "                                \"length\": np.float32,\n",
    "                                \"nav_status\": np.int32,\n",
    "                                \"RISK_Norwegian_Main_Vessel_Category_ID\": np.float32}\n",
    "                        )\n",
    "\n",
    "# Renaming fields\n",
    "AIS_df.rename(columns={\"date_time_utc\": \"Time\",\n",
    "                        \"mmsi\": \"ID\",\n",
    "                        \"lon\": \"LON\",\n",
    "                        \"lat\": \"LAT\",\n",
    "                        \"nav_status\": \"Status\",\n",
    "                        \"true_heading\": \"Heading\",\n",
    "                        \"sog\": \"SOG\",\n",
    "                        \"cog\": \"COG\",\n",
    "                        \"length\": \"Length\",\n",
    "                        \"RISK_Norwegian_Main_Vessel_Category_ID\": \"Category\"\n",
    "                         },inplace=True)\n",
    "\n",
    "# Converting time column from string to datetime\n",
    "AIS_df[\"Time\"] = pd.to_datetime(AIS_df.Time,format=\"%Y-%m-%dT%H:%M:%S.%f\",box=False)\n",
    "\n",
    "# Storing datetimes in the dataframe\n",
    "AIS_df[\"Time_datetime\"] = AIS_df[\"Time\"]\n",
    "\n",
    "# Converting datetimes to ints for later conversion to intervals\n",
    "AIS_df[\"Time\"] = AIS_df.Time.values.astype(np.uint64) / 10**9\n",
    "\n",
    "# Removing platforms\n",
    "AIS_df = AIS_df[(AIS_df.ID > 99999999)&(AIS_df.ID <= 999999999)]\n",
    "\n",
    "# Sorting by time\n",
    "AIS_df.sort_values(by = \"Time\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "AIS_df[\"Day\"] = 13\n",
    "AIS_df[\"Interval\"] = (AIS_df.Time - np.min(AIS_df.Time)) // 20\n",
    "AIS_df[\"Interval\"] = AIS_df.Interval.values.astype(np.uint32)\n",
    "AIS_df = AIS_df.drop_duplicates([\"ID\",\"Interval\"],keep=\"first\")\n",
    "AIS_df = AIS_df.sort_values(by=[\"Time\",\"LAT\",\"LON\"], axis=0, inplace=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing on the small dataset\n",
    "\n",
    "Computing happens here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AIS_filtered = time_to_CPA_calculator(AIS_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting observations\n",
    "DATABANK = observation_collector(AIS_df, AIS_filtered)\n",
    "\n",
    "AIS_databank = DATABANK\n",
    "\n",
    "# Synchronizing times\n",
    "databank_mergetime = observation_synchronizer(AIS_databank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculating Statistics (CPA, tCPA, CPA_dist) for all observations ###\n",
    "\n",
    "AIS_db = databank_mergetime\n",
    "\n",
    "# Calculating stats\n",
    "AIS_with_stats = Stat_calculator(AIS_db)\n",
    "\n",
    "# Calculating dSOG and dCOGs\n",
    "AIS_with_diffs = diffs_computer(AIS_with_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AIS_with_diffs = pd.io.parsers.read_csv(\"AIS_with_stats.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculating summary statistics ###\n",
    "AIS2_db = AIS_with_diffs\n",
    "\n",
    "# Calculating summart statistics\n",
    "AIS_summary_stats = statistics_aggregator(AIS2_db)\n",
    "\n",
    "AIS_with_diffs.to_csv(\"AIS_with_stats.csv\",index=False)\n",
    "\n",
    "# Writing final output for further analysis\n",
    "AIS_summary_stats.to_csv(\"AIS_summary_stats.csv\",index=False,\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading from parquet and writing to HDF5\n",
    "\n",
    "Reading and writing to queryable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.7/site-packages/pyarrow/pandas_compat.py:752: FutureWarning: .labels was deprecated in version 0.24.0. Use .codes instead.\n",
      "  labels, = index.labels\n"
     ]
    }
   ],
   "source": [
    "files = glob.glob(\"/users/arnsteinvestre/code/Parquet/*.parquet\")\n",
    "cols = [\"Timestamp\", \"MMSI-nummer\", \"IMO\", \"Longitude\", \"Latitude\", \"Speed_over_ground\", \"Course_over_ground\", \"Navigational_status\", \"Risk_category\",\"Width\"]\n",
    "pqdataset = pq.ParquetDataset(files)\n",
    "AIS_pqds = pqdataset.read(columns = cols)\n",
    "AIS_pd = AIS_pqds.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lengths_df = pd.read_pickle(\"vessel_imo_length.pkl\") \n",
    "Lengths_short_df = Lengths_df.drop_duplicates([\"IMO\"],keep=\"first\")\n",
    "Lengths_clean_df = Lengths_short_df[Lengths_short_df[\"LENGTHOVERALL\"] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lengths_clean_df = Lengths_clean_df[[\"IMO\",\"LENGTHOVERALL\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "AIS_pd = pd.merge(AIS_pd, Lengths_clean_df, on=\"IMO\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating day labels\n",
    "AIS_pd[\"Daystamp\"] = AIS_pd[\"Timestamp\"].dt.normalize()\n",
    "AIS_pd[\"Day\"] = AIS_pd[\"Daystamp\"].factorize()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming fields\n",
    "AIS_pd.drop(labels=\"Daystamp\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "AIS_pd.rename(columns={\"Timestamp\": \"Time\",\n",
    "                        \"MMSI-nummer\": \"ID\",\n",
    "                        \"Longitude\": \"LON\",\n",
    "                        \"Latitude\": \"LAT\",\n",
    "                        \"Navigational_status\": \"Status\",\n",
    "                        \"Speed_over_ground\": \"SOG\",\n",
    "                        \"Course_over_ground\": \"COG\",\n",
    "                        \"Length\": \"Length\",\n",
    "                        \"Risk_category\": \"Category\",\n",
    "                       \"LENGTHOVERALL\": \"Length\"\n",
    "                         },inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing datetimes in the dataframe\n",
    "AIS_pd[\"Time_datetime\"] = AIS_pd[\"Time\"]\n",
    "\n",
    "# Converting datetimes to ints for later conversion to intervals\n",
    "AIS_pd[\"Time\"] = AIS_pd.Time.values.astype(np.uint64) / 10**9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "AIS_pd[\"Length\"] = AIS_pd[\"Length\"].astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AIS_pd.drop(columns = [\"IMO\",\"Risk_category_name\",\"Ship_cargo_type_ID\"], inplace = True)\n",
    "# If not properly read (with cols restriction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 0 separated\n",
      "Day 0 written\n",
      "Day 1 separated\n",
      "Day 1 written\n",
      "Day 2 separated\n",
      "Day 2 written\n",
      "Day 3 separated\n",
      "Day 3 written\n",
      "Day 4 separated\n",
      "Day 4 written\n",
      "Day 5 separated\n",
      "Day 5 written\n",
      "Day 6 separated\n",
      "Day 6 written\n",
      "Day 7 separated\n",
      "Day 7 written\n",
      "Day 8 separated\n",
      "Day 8 written\n",
      "Day 9 separated\n",
      "Day 9 written\n",
      "Day 10 separated\n",
      "Day 10 written\n",
      "Day 11 separated\n",
      "Day 11 written\n"
     ]
    }
   ],
   "source": [
    "data_store = pd.HDFStore(\"AIS_from_parquet_large.h5\")\n",
    "\n",
    "for day in range(0,12):\n",
    "\n",
    "    AIS_daytable = AIS_pd[AIS_pd[\"Day\"] == day]\n",
    "    \n",
    "    print(\"Day\",day,\"separated\")\n",
    "    \n",
    "    data_store[\"day{}_df\".format(day)] = AIS_daytable\n",
    "    \n",
    "    print(\"Day\",day,\"written\")\n",
    "\n",
    "data_store.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting instances on the large set\n",
    "\n",
    "We now transition to reading the large file and turning this into a larger databank.\n",
    "This requires using a larger part of memory, thus be careful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First some testing\n",
    "\n",
    "We need to check that it works on a small sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_store = pd.HDFStore(\"AIS_from_parquet_large.h5\")\n",
    "AIS_pd = data_store[\"day1_df\".format(day)]\n",
    "data_store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "AIS_pd_samp = AIS_pd[np.random.rand(len(AIS_pd.index)) < 0.05].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating intervals\n",
    "AIS_pd_samp[\"Interval\"] = (AIS_pd_samp.Time - np.min(AIS_pd_samp.Time)) // 20\n",
    "AIS_pd_samp[\"Interval\"] = AIS_pd_samp.Interval.values.astype(np.uint32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "AIS_shorter = AIS_pd_samp.drop_duplicates([\"ID\",\"Interval\"],keep=\"first\")\n",
    "AIS_shorter = AIS_shorter.sort_values(by=[\"Time\",\"LON\",\"LAT\"], axis=0, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "AIS_filtered = time_to_CPA_calculator(AIS_shorter,namedict = {\"DateTime\": \"Time_datetime\", \n",
    "                            \"ID\": \"ID\",\n",
    "                            \"Time\": \"Time\",\n",
    "                            \"COG\": \"COG\",\n",
    "                            \"SOG\": \"SOG\",\n",
    "                            \"Heading\": \"Heading\",\n",
    "                            \"LON\": \"LON\",\n",
    "                            \"LAT\": \"LAT\",\n",
    "                            \"Status\": \"Status\",\n",
    "                            \"Length\": \"Length\",\n",
    "                            \"Interval\": \"Interval\",\n",
    "                            \"Distances\": \"Distances\",\n",
    "                            \"Day\": \"Day\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AIS_collector = AIS_filtered\n",
    "\n",
    "# Collecting observations\n",
    "DATABANK_parq = observation_collector(AIS_pd_samp, AIS_filtered)\n",
    "\n",
    "# Synchronizing times\n",
    "databank_mergetime_l = observation_synchronizer(DATABANK_parq)\n",
    "# Computing time to cpas and differentiations\n",
    "AIS_large_with_stats = Stat_calculator(databank_mergetime_l)\n",
    "AIS_large_with_diffs = diffs_computer(AIS_large_with_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing summary stats for each situation\n",
    "AIS_large_summary_stats = statistics_aggregator(AIS_large_with_diffs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now it starts for real\n",
    "This is a loop that ideally computes for the whole set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting calculations day 8\n",
      "Finished day 8\n",
      "Starting calculations day 9\n",
      "Finished day 9\n",
      "Starting calculations day 10\n",
      "Finished day 10\n",
      "Starting calculations day 11\n",
      "Finished day 11\n"
     ]
    }
   ],
   "source": [
    "### Read whole HDF5 table\n",
    "data_store = pd.HDFStore(\"AIS_from_parquet_large.h5\")\n",
    "day1 = 0\n",
    "AIS_collector_flag = False\n",
    "\n",
    "data_store2 = pd.HDFStore(\"AIS_large_instances.h5\")\n",
    "\n",
    "for day in range(day1,12):\n",
    "    \n",
    "    AIS_pd = data_store[\"day{}_df\".format(day)]\n",
    "    \n",
    "    AIS_pd[\"Time_Datetime\"] = AIS_pd[\"Time_datetime\"]\n",
    "    AIS_pd.drop(columns=[\"Time_datetime\"],inplace=True)\n",
    "    # Removing platforms\n",
    "    AIS_pd = AIS_pd[(AIS_pd.ID > 99999999)&(AIS_pd.ID <= 999999999)]\n",
    "\n",
    "    # Creating intervals\n",
    "    AIS_pd[\"Interval\"] = (AIS_pd.Time - np.min(AIS_pd.Time)) // 20\n",
    "    AIS_pd[\"Interval\"] = AIS_pd.Interval.values.astype(np.uint32)\n",
    "    \n",
    "    AIS_shorter = AIS_pd.drop_duplicates([\"ID\",\"Interval\"],keep=\"first\")\n",
    "    AIS_shorter = AIS_shorter.sort_values(by=[\"Time\",\"LON\",\"LAT\"], axis=0, inplace=False)\n",
    "    \n",
    "    print(\"Starting calculations day\",day)\n",
    "    \n",
    "    AIS_filtered = time_to_CPA_calculator(AIS_shorter)\n",
    "\n",
    "    data_store2[\"Large_instances_day_{}\".format(day)] = AIS_filtered\n",
    "    \n",
    "    if not AIS_collector_flag:\n",
    "        AIS_collector = AIS_filtered\n",
    "        AIS_collector_flag = True\n",
    "    else:\n",
    "        AIS_collector = pd.concat([AIS_collector, AIS_filtered])\n",
    "    \n",
    "    print(\"Finished day\",day)\n",
    "data_store.close()\n",
    "\n",
    "print(\"Storing\")\n",
    "# Lagre mellomlagring\n",
    "data_store2[\"Large_instances_all\"] = AIS_collector\n",
    "data_store2.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using instances to collect databank from large set\n",
    "\n",
    "The databank is collected here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 1226 erroneous records\n",
      "Removing 2887 inf entitites\n",
      "Removing 0 NaN entitites\n",
      "Finished day 0\n",
      "Removing 1516 erroneous records\n",
      "Removing 6637 inf entitites\n",
      "Removing 0 NaN entitites\n",
      "Finished day 1\n",
      "Removing 1595 erroneous records\n",
      "Removing 3591 inf entitites\n",
      "Removing 0 NaN entitites\n",
      "Finished day 2\n",
      "Removing 120175 erroneous records\n",
      "Removing 10407 inf entitites\n",
      "Removing 0 NaN entitites\n",
      "Finished day 3\n",
      "Removing 724 erroneous records\n",
      "Removing 3078 inf entitites\n",
      "Removing 0 NaN entitites\n",
      "Finished day 4\n",
      "Removing 5619 erroneous records\n",
      "Removing 4813 inf entitites\n",
      "Removing 0 NaN entitites\n",
      "Finished day 5\n",
      "Removing 706 erroneous records\n",
      "Removing 3366 inf entitites\n",
      "Removing 0 NaN entitites\n",
      "Finished day 6\n",
      "Removing 1353 erroneous records\n",
      "Removing 3948 inf entitites\n",
      "Removing 0 NaN entitites\n",
      "Finished day 7\n",
      "Removing 4848 erroneous records\n",
      "Removing 21607 inf entitites\n",
      "Removing 0 NaN entitites\n",
      "Finished day 8\n",
      "Removing 863 erroneous records\n",
      "Removing 2129 inf entitites\n",
      "Removing 0 NaN entitites\n",
      "Finished day 9\n",
      "Removing 834 erroneous records\n",
      "Removing 5734 inf entitites\n",
      "Removing 0 NaN entitites\n",
      "Finished day 10\n",
      "Removing 1320 erroneous records\n",
      "Removing 4496 inf entitites\n",
      "Removing 0 NaN entitites\n",
      "Finished day 11\n"
     ]
    }
   ],
   "source": [
    "data_storeA = pd.HDFStore(\"AIS_large_istances_day0to7.h5\")\n",
    "data_storeB = pd.HDFStore(\"AIS_large_istances_8to11.h5\")\n",
    "\n",
    "AIS_large_filtered1 = pd.concat([data_storeA[\"Large_instances_day0to7\"], data_storeB[\"Large_instances_8to11\"]])\n",
    "AIS_databank_flag = False\n",
    "\n",
    "data_storeA.close()\n",
    "data_storeB.close()\n",
    "\n",
    "data_store = pd.HDFStore(\"AIS_from_parquet_large.h5\")\n",
    "\n",
    "data_storeSTAT = pd.HDFStore(\"AIS_summary_stats_large_backup.h5\")\n",
    "\n",
    "for day in range(0,12):\n",
    "    AIS_pd = data_store[\"day{}_df\".format(day)]\n",
    "    \n",
    "    # Collecting observations\n",
    "    DATABANK_parq = observation_collector(AIS_pd, AIS_large_filtered1[AIS_large_filtered1[\"Day\"] == day])\n",
    "    \n",
    "    # Synchronizing times\n",
    "    databank_mergetime_l = observation_synchronizer(DATABANK_parq)\n",
    "    \n",
    "    # Computing time to cpas and differentiations\n",
    "    AIS_large_with_stats = Stat_calculator(databank_mergetime_l)\n",
    "    AIS_large_with_diffs = diffs_computer(AIS_large_with_stats)\n",
    "    \n",
    "    # Saving stat-ed file\n",
    "    AIS_large_with_diffs.to_csv(\"AIS_large_with_stats_day{}.csv\".format(day),\n",
    "                                index=False)\n",
    "    \n",
    "    # Computing summary stats for each situation\n",
    "    AIS_large_summary_stats = statistics_aggregator(AIS_large_with_diffs)\n",
    "\n",
    "    data_storeSTAT[\"Stats_day_{}\".format(day)] = AIS_large_summary_stats\n",
    "    \n",
    "    if not AIS_databank_flag:\n",
    "        AIS_stats_databank_large = AIS_large_summary_stats\n",
    "        AIS_databank_flag = True\n",
    "    else:\n",
    "        AIS_stats_databank_large = pd.concat([AIS_stats_databank_large,AIS_large_summary_stats])\n",
    "    \n",
    "    print(\"Finished day\",day)\n",
    "\n",
    "# Writing final output for further analysis\n",
    "data_store.close()\n",
    "\n",
    "AIS_stats_databank_large = AIS_stats_databank_large.reset_index(drop = True)\n",
    "AIS_stats_databank_large[\"Situation_glob\"] = AIS_stats_databank_large.index + 1\n",
    "\n",
    "AIS_stats_databank_large.to_csv(\"AIS_large_summary_stats.csv\",index=False,\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
